""" Module comprising `mindcraft`-related `PyTorch` utility functions.

(c) B. Hartl 2021
"""
import torch
from numpy import ndim, product, ndarray, asarray
from torch import Tensor, tensor, no_grad
from torch import float as torch_float
from torch import nn
import torch.nn.functional as F
from torch.nn.init import _calculate_fan_in_and_fan_out
from typing import Union
import warnings
import math
from collections import OrderedDict
from collections import namedtuple
from numpy import mod as np_mod
import numpy as np


###############################################################################################################
def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def variance_scaling_(tensor, scale=1.0, mode='fan_in', distribution='normal'):
    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)
    if mode == 'fan_in':
        denom = fan_in
    elif mode == 'fan_out':
        denom = fan_out
    elif mode == 'fan_avg':
        denom = (fan_in + fan_out) / 2

    variance = scale / denom

    if distribution == "truncated_normal":
        # constant is stddev of standard normal truncated to (-2, 2)
        trunc_normal_(tensor, std=math.sqrt(variance) / .87962566103423978)
    elif distribution == "normal":
        tensor.normal_(std=math.sqrt(variance))
    elif distribution == "uniform":
        bound = math.sqrt(3 * variance)
        tensor.uniform_(-bound, bound)
    else:
        raise ValueError(f"invalid distribution {distribution}")


def lecun_normal_(tensor):
    return variance_scaling_(tensor, mode='fan_in', distribution='truncated_normal')


def adapt_input_conv(in_chans, conv_weight):
    conv_type = conv_weight.dtype
    conv_weight = conv_weight.float()  # Some weights are in torch.half, ensure it's float for sum on CPU
    O, I, J, K = conv_weight.shape
    if in_chans == 1:
        if I > 3:
            assert conv_weight.shape[1] % 3 == 0
            # For models with space2depth stems
            conv_weight = conv_weight.reshape(O, I // 3, 3, J, K)
            conv_weight = conv_weight.sum(dim=2, keepdim=False)
        else:
            conv_weight = conv_weight.sum(dim=1, keepdim=True)
    elif in_chans != 3:
        if I != 3:
            raise NotImplementedError('Weight format not supported by conversion.')
        else:
            # NOTE this strategy should be better than random init, but there could be other combinations of
            # the original RGB input layer weights that'd work better for specific cases.
            repeat = int(math.ceil(in_chans / 3))
            conv_weight = conv_weight.repeat(1, repeat, 1, 1)[:, :in_chans, :, :]
            conv_weight *= (3 / float(in_chans))
    conv_weight = conv_weight.to(conv_type)
    return conv_weight
###############################################################################################################


def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):
    """ Utility function for computing output of convolutions
        takes a number h_w or a tuple of (h,w) and returns a number h_w or a tuple of (h,w)

    see `pytorch discussion <https://discuss.pytorch.org/t/utility-function-for-calculating-the-shape-of-a-conv-output/11173/6>`_
    """

    if ndim(h_w) > 0 and not isinstance(kernel_size, tuple):
        kernel_size = tuple([kernel_size] * len(h_w))

    if ndim(h_w) > 0 and not isinstance(stride, tuple):
        stride = tuple([stride] * len(h_w))

    if ndim(h_w) > 0 and not isinstance(pad, tuple):
        pad = tuple([pad] * len(h_w))

    if not hasattr(h_w, '__len__'):
        h_w_prime = (h_w + (2 * pad) - (dilation * (kernel_size - 1)) - 1) // stride + 1

    else:
        h_w_prime = tuple([
            (h_w[i] + (2 * pad[i]) - (dilation * (kernel_size[i] - 1)) - 1) // stride[i] + 1
            for i in range(len(h_w))
        ])

    return h_w_prime


def conv_transpose_output_shape(h_w, kernel_size=1, stride=1, pad=0):
    """ Utility function for computing output of transposed convolutions
        takes a number h_w or a tuple of (h,w) and returns a number h_w or a tuple of (h,w)

    see `pytorch discussion <https://discuss.pytorch.org/t/utility-function-for-calculating-the-shape-of-a-conv-output/11173/6>`_
    """

    if ndim(h_w) > 0 and not isinstance(kernel_size, tuple):
        kernel_size = tuple([kernel_size] * len(h_w))

    if ndim(h_w) > 0 and not isinstance(stride, tuple):
        stride = tuple([stride] * len(h_w))

    if ndim(h_w) > 0 and not isinstance(pad, tuple):
        pad = tuple([pad] * len(h_w))

    if not hasattr(h_w, '__len__'):
        # h_w_prime = (h_w - 1) * stride - 2 * pad + kernel_size + pad  # keras ?
        h_w_prime = (h_w - 1) * stride - 2 * pad + kernel_size          # torch

    else:
        h_w_prime = tuple([
            # (h_w[i] - 1) * stride[i] - 2 * pad[i] + kernel_size[i] + pad[i]  # keras ?
            (h_w[i] - 1) * stride[i] - 2 * pad[i] + kernel_size[i]             # torch
            for i in range(len(h_w))
        ])

    return h_w_prime


def get_layer_nd(dim, layer_prefix='Conv'):
    """ get an n-dimensional layer with a given layer_prefix (e.g. 'Conv', 'ConvTranspose' or 'BatchNorm',
        from the torch.nn module by specifying the dimension. """

    if len(dim) == 1:
        layer_nd = getattr(nn, f'{layer_prefix}1d')

    elif len(dim) == 2:
        layer_nd = getattr(nn, f'{layer_prefix}2d')

    elif len(dim) == 3:
        layer_nd = getattr(nn, f'{layer_prefix}3d')

    else:
        raise AssertionError("input_shape must be in (1, 2, 3)")

    return layer_nd


def get_closest(x: Tensor, values: list):
    """ get the closest value of elements in x in a list of values."""
    values = torch.Tensor(sorted(values))
    abs_diff = torch.abs(x[:, None] - values[None, :])
    return values[abs_diff.argmin(dim=1)]


def get_conv_nd(dim):
    """ get an n-dimensional Conv layer from the torch.nn module by specifying the dimension. """
    return get_layer_nd(dim, layer_prefix='Conv')


def get_conv_transpose_nd(dim):
    """ get an n-dimensional ConvTranspose layer from the torch.nn module by specifying the dimension. """
    return get_layer_nd(dim, layer_prefix='ConvTranspose')


def get_batch_norm_nd(dim):
    """ get an n-dimensional BatchNorm layer from the torch.nn module by specifying the dimension. """
    return get_layer_nd(dim, layer_prefix='BatchNorm')


def get_torch_layer(layer_type: Union[str, type]):
    """ Retrieve PyTorch module from str representation. """
    if isinstance(layer_type, str):
        layer = getattr(nn, layer_type, None)
        if layer is not None:
            return layer

        from mindcraft.torch import layer
        try:
            return getattr(layer, layer_type)
        except AttributeError:
            print("here")
            raise

    assert isinstance(layer_type, nn.Module) or layer_type is nn.Identity, f"Unknown `layer_type` '{layer_type}'."
    return layer_type


def get_activation_function(activation: str):
    """ get a callable activation function from the torch framework or from the torch.nn.functional
    module by str representation. Per default and on error None is returned. """
    if isinstance(activation, type):
        return activation
    try:
        foo = getattr(torch, activation, getattr(torch.nn, activation, getattr(F, activation, None)))
        return foo
    except TypeError:
        return None


def layer_type_repr(layer_type):
    if isinstance(layer_type, str):
        return layer_type

    if isinstance(layer_type, type):
        return layer_type.__name__

    return layer_type.__class__.__name__


def flatten_parameters(model, calc_indices=True, mask=None):
    """ flattens all parameters into a single column vector. Returns the dictionary to recover them

    :param: model: model hosting parameters in a state_dict
    :param calc_indices: boolean, specifying whether to evaluate the indices
                         of the `state_dict` parameters in the flattened parameters
                         and store them in a {param_key: (start_index, stop_index)} dictionary.
    :param mask: Optional mapping of {param_key: param_mask} values with boolean indexer array `param_mask`.
                 specified masked parameters are flattened as `flat_params[start_i, stop_i] = param[mask]`.
    :return: The function returns a tuple of
             (flattened parameters, [key, (start index, end index) for each param in the model's state_dict])
             if `calc_indices is True` (**Note end index in not inclusive**),
             or only the flattened parameters otherwise.

    adapted from https://discuss.pytorch.org/t/how-to-flatten-and-then-unflatten-all-model-parameters/34730/2
    """
    if mask is None:
        mask = {}

    flattened_p = [(torch.flatten(p) if k not in mask else p[mask[k]])
                   for k, p in model.state_dict().items()]

    try:
        flat_p = torch.cat(flattened_p).reshape(-1, )
    except RuntimeError:
        flat_p = torch.empty(0)

    if not calc_indices:
        return flat_p

    key_indices = []
    s = 0
    for k, p in model.state_dict().items():
        if k in mask:
            size = len(p[mask[k]])
        else:
            size = int(product(p.shape))
        key_indices.append((k, (s, s+size)))
        s += size

    return flat_p, key_indices


def recover_flattened(flat_parameters, indices, model, update_model=False, mask=None):
    """ Gives a list of recovered parameters from their flattened form
    
    :param flat_parameters: [#params, 1]
    :param indices: a list of start and end index of each param [(start, end) for param]
    :param model: the model hosting the params with correct shapes
    :param update_model:
    :param mask: Optional mapping of {param_key: param_mask} values with boolean indexer array `param_mask`.
                 Specified masked parameters are filled according to `param[mask] = flat_params[start_i, stop_i]`.
    :return: the params, reshaped to the ones in the model, with the same order as those in the model

    adapted from https://discuss.pytorch.org/t/how-to-flatten-and-then-unflatten-all-model-parameters/34730/2
    """
    if mask is None:
        mask = {}

    recovered_p = []
    for k, (s, e) in indices:
        p = model.state_dict()[k]
        try:
            assert p.shape
            recovered_se = flat_parameters[s:e]
            if k not in mask:
                recovered_se = recovered_se.view(*p.shape)

            if update_model:
                i = slice(None) if k not in mask else mask[k]
                model.state_dict()[k][i] = recovered_se

        except AssertionError:
            assert e - s == 1  # 0
            recovered_se = torch.Tensor(flat_parameters[s:e][0])
            if update_model:
                model.state_dict()[k].fill_(recovered_se)

        except TypeError:
            raise

        recovered_p.append(recovered_se)

    return recovered_p


def set_parameters(state_dict: Union[dict, OrderedDict], module: nn.Module, partial_ok: bool = True) -> nn.Module:
    """ Copies all key-value pair parameters from `state_dict` to the corresponding named parameter in `module`.

    :param state_dict: Source `dict` or `OrderedDict` from which parameters are copied into `module`.
    :param module: Destination PyTorch `Module` instance to copy `state_dict`'s parameters into.
    :param partial_ok: Boolean specifying whether not all variables in the `module`'s state_dict need to be attributed
                       via the `state_dict` argument, defaults to True (i.e., a partial parameter specification via
                       the `state_dict` argument is ok). If False, a `KeyError` might be raised if a parameter name
                       of `module` does not appear in the `state_dict` argument.
    :return: The updated PyTorch module.
    """
    for k, v in module.state_dict().items():
        if k in state_dict:
            v[...] = state_dict[k]
        elif not partial_ok:
            raise KeyError(f"Module-parameter named '{k}' not found in `state_dict`.")

    return module


def tensor_to_numpy(tensor):
    detached_action = tensor.detach()
    try:
        return detached_action.numpy()
    except RuntimeError:  # grad
        return detached_action.detach().numpy()
    except TypeError:  # gpu
        return detached_action.cpu().numpy()


def to_tensor(array_like, device='cpu'):
    array_like = asarray(array_like)
    tensor = torch.Tensor(array_like)
    return tensor.to(device)


def concatenate(x, dim=-1):
    return torch.cat(x, dim=dim)


def get_n_params(model):
    pp = 0
    for p in list(model.parameters()):
        nn = 1
        for s in list(p.size()):
            nn = nn * s
        pp += nn
    return pp


def get_conv2d_output_size(input_size, kernel_size, stride, padding=0, dilation=1):
    """ Return the output size of a Convolution2D layer, given the input size and other Conv2D parameters

        see:
            - https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html
            - https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html
    """
    padding = padding or 0
    return (input_size + 2*padding - dilation * (kernel_size-1) - 1)/stride + 1


def get_conv_transpose2d_output_size(input_size, kernel_size, stride, padding=0, out_padding=0, dilation=1):
    """ Return the output size of a Convolution2D layer, given the input size and other Conv2D parameters

        see:
            - https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html
            - https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html
    """
    return (input_size - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + out_padding + 1


class AutoGrad:
    def __init__(self, model, lr=1e-4, steps=1, device='cpu', descent=True):
        """ Constructs a `AutoGrad` instance

        :param model: The pytorch module.
        :param lr: The learning rate when gradient updates are performed, defaults to 1e-4.
        :param steps: The number of gradient update steps during sampling, defaults to 1.
        :param device: Device for model evaluation and training ('cpu' or 'cuda').
        :param dtype: A torch datatype when specifying tenr
        """
        self.model = model
        self.lr = abs(lr) if descent else -abs(lr)
        self.steps = steps
        self.device = device

        # helpers:
        self.t = 1

    def __call__(self, x, clone=False):
        assert isinstance(x, Tensor)
        if clone:
            x = x.clone()

        x = x.to(self.device)
        x.requires_grad = True

        # Perform Backpropagation with x to optimize the loss function
        for i in range(self.steps):
            y = self.model(x)
            self.model.zero_grad()

            # Compute the loss and gradients for each batch element
            x_prime = x.clone().detach()
            cost = self.loss(y)

            for j in range(len(cost)):  # iterate batch-dim
                # Compute the loss as the j-th element of the module output
                loss = cost[j]

                # Compute the gradient for the j-th element
                loss.backward(retain_graph=True)

                # Update the input for the j-th element
                with no_grad():
                    x_prime[j] += self._compute_step_j(x, j)

                # Zero the gradient for the next iteration
                x.grad.zero_()

            x = x_prime
            x.requires_grad = True
            self.t += 1

        return x, self.model(x), {}

    def _compute_step_j(self, x, j):
        return -self.lr * x.grad[j]

    def loss(self, y):
        if len(y.shape) == 1:
            return y
        else:
            return y.mean(dim=-1)

    @classmethod
    def sg(cls, model, lr=1e-4, momentum=0.9, steps=1, device='cpu', dtype=torch_float, descent=True):
        """ stochastic gradient descent[`descent == True`] or ascent[`descent == False`] """
        class AutoSGD(AutoGrad):
            def __init__(self, momentum=0.9, **kwargs):
                self.momentum = momentum
                self.v = None
                AutoGrad.__init__(self, **kwargs)

            def __call__(self, x, clone=False):
                if self.v is None:
                    self.v = torch.zeros_like(x)
                return AutoGrad.__call__(self, x, clone=clone)

            def _compute_step_j(self, x, j):
                self.v[j] = self.momentum * self.v[j] + (1. - self.momentum) * x[j]
                step_j = -self.lr * self.v[j]
                return step_j

        opt = AutoSGD(model=model, lr=lr, momentum=momentum, steps=steps, device=device, dtype=dtype, descent=descent)
        return opt

    @classmethod
    def adam(cls, model, lr=1e-4, beta1=0.99, beta2=0.999, steps=1, epsilon=1e-8, device='cpu', dtype=torch_float, descent=True):
        """ adam gradient descent[`descent == True`] or ascent[`descent == False`] """
        class AutoAdam(AutoGrad):
            def __init__(self, beta1=0.99, beta2=0.999, epsilon=1e-8, **kwargs):
                self.beta1 = beta1
                self.beta2 = beta2
                self.epsilon = epsilon
                self.m = None
                self.v = None
                AutoGrad.__init__(self, **kwargs)

            def __call__(self, x, clone=False):
                if self.m is None:
                    self.m = torch.zeros_like(x)
                    self.v = torch.zeros_like(x)
                return AutoGrad.__call__(self, x, clone=clone)

            def _compute_step_j(self, x, j):
                a = self.lr * math.sqrt(1 - self.beta2 ** self.t) / (1 - self.beta1 ** self.t)
                self.m[j] = self.beta1 * self.m[j] + (1 - self.beta1) * x[j]
                self.v[j] = self.beta2 * self.v[j] + (1 - self.beta2) * (x[j] * x[j])
                step_j = -a * self.m[j] / (torch.sqrt(self.v[j]) + self.epsilon)
                return step_j

        opt = AutoAdam(model=model, lr=lr, beta1=beta1, beta2=beta2, epsilon=epsilon, steps=steps,
                       device=device, dtype=dtype, descent=descent)
        return opt


class REMC:
    """ A Replica Exchange Monte Carlo sampling (or Parallel Tempering) implementation

        - to maximize the model output
        - by sampling the input.
        """
    def __init__(self, model, n_params=None, n_samples=None, beta_low=1e-8, beta_high=1e8, num_beta=128, lin_beta=False,
                 sigma=1., tempering_steps=10, sampling_steps=100, device='cpu', stateful=False, comm=None):
        """ Constructs a `REMC` instance

        :param model: The pytorch module.
        :param n_samples: The batch-size, defaults to None (in that case, `n_samples` is initialized from `x0.shape[0]`
                          when the sampler is `__call__`ed).
        :param n_params: The input size of the model, defaults to None (in that case, `n_params` is initialized from
                         `x0.shape[1]` when the sampler is `__call__`ed).
        :param beta_high: In case of `TEMPERING`-`sampling_method` this represents the upper limit of the beta range
                          i.e., the lower limit of the temperature (=1/beta)  range for parallel tempering.
        :param beta_low: In case of `TEMPERING`-`sampling_method` this represents the lower limit of the beta range
                         i.e., the upper limit of the temperature (=1/beta) range for parallel tempering.
                         In case of `GRADIENT` it also represents the stepsize for backpropagation optimization of
                         "improving" the most recent evolutionary strategy's generation parameters.
        :param num_beta: In case of `TEMPERING`-`sampling_method` this represents the number of logarithmic beta
                         increments between `beta_high`and `beta_low`.
        :param lin_beta: Boolean flag to use linear beta grid (if True), defaults to False: log beta grid.
        :param sigma: The width of Gaussian noise used during sampling initial parameter-samples, during or
                      Monte Carlo sweeps case of `REMC` - `sampling_method`.
        :param tempering_steps: The number of tempering steps (exchanging samples in temperature (or beta) array).
        :param sampling_steps:  The number of sampling steps at a fixed temperature/beta.
        :param device: Device for model evaluation and training ('cpu' or 'cuda').
        :param stateful: Boolean flag to enable a "running" sampling from the most recent samples after each
                         tempering run (if True). Otherwise, each tempering will start from `x0` (if not None)
                         or from a randomly initialized set of parameters.
        :param comm: An optional MPI communicator, defaults to None.
        """
        self.model = model
        self.n_params = n_params
        self.n_samples = n_samples

        self.beta_low = beta_low
        self.beta_high = beta_high
        self.num_beta = num_beta
        self.lin_beta = lin_beta
        self.sigma = sigma

        self.stateful = stateful
        self.device = device
        self.tempering_steps = tempering_steps
        self.sampling_steps = sampling_steps

        self.gathered_info = {}
        if comm is None:
            from collections import namedtuple
            MPIComm = namedtuple("comm", "Get_rank Get_size bcast gather")
            comm = MPIComm(lambda: 0,            # rank
                           lambda: 1,            # size
                           lambda x, root=0: x,  # dummy bcast
                           lambda x, root=0: x,  # dummy gather
                           )
        self.comm = comm    

    def __call__(self, x0=None, **kwargs):
        x = None                  # running sample
        x0 = self._init_x0(x0)    # optional initial guess
        beta = self._init_beta()  # inverse temperature grid
        samples, samples_info, rewards = [], {}, []
        for s in self._sample_idx:
            # INITIALIZE
            x, cost, tempering_info = self._init_sampling(x, x0)

            # PERFORM >>> ONE <<< PARALLEL TEMPERING RUN
            for t in range(self.tempering_steps):
                x, cost, mc_info = self.metropolis(x, beta=beta)
                x, cost, tmp_info = self.tempering(x, cost=cost, step=t, beta=beta)

                # BOOKKEEPING
                self._add_tempring_stat(tempering_info, mc_info)
                self._add_tempring_stat(tempering_info, tmp_info)

            # CHECK FOR DUPLICATES
            sample, reward = self._check_duplicate(x, samples, cost, beta)

            # ADD TO SAMPLES
            samples.append(sample)
            rewards.append(reward)

            # STATISTICS
            self._add_sampling_stat(samples_info, tempering_info)

        # GATHER DATA AMONG DIFFERENT PROCESSES
        samples = self.comm.gather(samples, root=0)
        rewards = self.comm.gather(rewards, root=0)
        samples_info = self.comm.gather(samples_info, root=0)

        if self.comm.Get_rank() == 0:
            # REFORMATTING DATA-LISTS
            samples = torch.stack(samples)
            rewards = torch.tensor(rewards, device=self.device)
            samples_info = {k: torch.stack(v, dim=0) for k, v in samples_info.items()}

        samples = self.comm.bcast(samples, root=0)
        rewards = self.comm.bcast(rewards, root=0)
        info = self.comm.bcast(samples_info, root=0)
        self.update_tempering_parameters(info)
        return samples, rewards, info

    def metropolis(self, x0, beta):
        # INIT x0
        if x0 is None:
            x0 = torch.randn((self.num_beta, self.n_params), device=self.device, dtype=torch_float) * self.sigma

        # transform to torch tensor
        if not isinstance(x0, Tensor):
            x0 = tensor(x0, device=self.device, dtype=torch_float)

        # if single x0 was provided -> expand and vary all but one
        if len(x0.shape) == 1:
            var_x = torch.randn((self.num_beta - 1, self.n_params), device=self.device, dtype=torch_float)
            x0 = x0.repeat(self.num_beta, 1)
            x0[:-1] += var_x * self.sigma

        with no_grad():
            x = x0.clone()
            y = -self.model(x)  # reward maximization to energy minimization
            info = {'sweep': torch.zeros(self.num_beta, device=self.device)}

            for i in range(self.sampling_steps):
                x_hat, sweep_ids = self.sweep(x)
                y_hat = -self.model(x_hat)  # if not descent: reward maxim. to energy minim.
                accept = self.detailed_balance(old_value=y, new_value=y_hat, beta=beta)
                x[accept] = x_hat[accept]
                y[accept] = y_hat[accept]
                info['sweep'] += accept

            info['sweep'] = info['sweep'] / self.sampling_steps
            return x, -self.model(x), info

    def sweep(self, x: Tensor):
        sweep_ids = torch.randint(0, self.n_params, size=(self.num_beta,), device=self.device)
        x_hat = x.clone()
        x_hat[:, sweep_ids] += torch.randn(self.num_beta, device=self.device) * self.sigma
        return x_hat, sweep_ids

    @staticmethod
    def detailed_balance(old_value: tensor, new_value: tensor, beta: tensor):
        d = old_value.device
        accept = torch.zeros(len(old_value), dtype=torch.bool, device=d)

        zero_temp = torch.isinf(beta)
        if any(zero_temp):
            accept[zero_temp] = (old_value[zero_temp] > new_value[zero_temp]).flatten()

        inf_temp = (beta == 0.)
        if any(inf_temp):
            accept[inf_temp] = True

        # return numpy.random.rand() > min(1., numpy.exp((new_evaluation-old_evaluation)*beta))
        valid = ~zero_temp & ~inf_temp
        rands = torch.rand((sum(valid), 1), device=d)
        crit = (rands < torch.minimum(torch.exp(-(new_value[valid] - old_value[valid]) * beta[valid, None]), tensor(1., device=d)))
        accept[valid] = (crit.prod(dim=-1) == 1)
        return accept

    def tempering(self, x: Tensor, cost: Tensor, step: int, beta: Tensor):
        """ note: method from B. Hartl's `atuin` framework"""
        # all even steps look for T_i -> T_i+1, all odd steps for T_i+1 -> T_i+2
        slice_i = slice(np_mod(step, 2), -1, 2)
        slice_j = slice(np_mod(step, 2) + 1, None, 2)

        d_cost = cost[slice_i] - cost[slice_j]
        d_beta = beta[slice_i] - beta[slice_j]
        r = torch.rand((len(d_beta), 1), device=x.device)

        swap: Tensor = d_cost < 0.  # accept if lower energy, i.e., (c_i < c_j) or (c_i - c_j < 0.)
        swap |= (r <= torch.minimum(torch.exp(d_cost * d_beta[:, None]), tensor(1., device=x.device)))  # or by detailed balance
        swap = swap.prod(dim=-1) == 1  # reduce for multiple fitness dimensions

        sample_idx = torch.arange(0, self.num_beta, device=self.device)
        i, j = sample_idx[slice_i][swap], sample_idx[slice_j][swap]
        if any(i):
            x[i], x[j] = x[j], x[i]

        return x, cost, {'swap': swap.type(torch.float)}

    def update_tempering_parameters(self, info):
        # TODO: step-size adaptation, etc
        pass

    def _init_x0(self, x0):
        if x0 is not None:
            x0 = self.comm.bcast(x0, root=0)

            # assure tensor
            if not isinstance(x0, Tensor):
                x0 = tensor(x0, device=self.device)

            # assure 2d
            if len(x0.shape) == 1:
                x0 = x0[None, :]

            assert len(x0.shape) == 2

            if self.n_samples is None:
                self.n_samples = x0.shape[0]

            if self.n_params is None:
                self.n_params = x0.shape[1]

            # assure shape [num_beta, ...]
            if x0.shape[0] < self.num_beta:
                l0 = x0.shape[0]
                x0 = x0.repeat(math.ceil(self.num_beta / x0.shape[0]), *[1] * (len(x0.shape) - 1))
                x0[l0:] += torch.randn_like(x0[l0:]) * self.sigma

            x0 = x0[:self.num_beta]

        return x0

    def _init_beta(self):
        if not self.lin_beta:
            low, high = math.log10(self.beta_low), math.log10(self.beta_high)
            return torch.logspace(low, high, self.num_beta, device=self.device)

        return torch.linspace(self.beta_low, self.beta_high, self.num_beta, device=self.device)

    def _check_duplicate(self, x, samples, cost, beta):
        sample = None
        reward = -torch.inf
        beta_order = torch.argsort(beta)
        check_duplicate = self.num_beta
        while check_duplicate:
            high_beta_idx = beta_order[check_duplicate - 1]
            sample = x[high_beta_idx]
            reward = -cost[high_beta_idx]
            duplicate = False
            for other in samples:
                if torch.equal(sample, other):
                    duplicate = True
                    break
            if not duplicate:
                break
            check_duplicate -= 1

        return sample, reward

    def _init_sampling(self, x, x0):
        cost = None
        sample_info = {}
        if x is None or not self.stateful:
            if x0 is not None:
                x = x0.clone()

        return x, cost, sample_info

    @property
    def _sample_idx(self):
        from numpy import array_split, arange
        sample_idx = array_split(arange(self.n_samples), self.comm.Get_size())[self.comm.Get_rank()]
        return sample_idx

    @staticmethod
    def _add_tempring_stat(info_dict, sampling_info):
        for k, v in sampling_info.items():
            col = info_dict.get(k, [])
            col.append(v)
            info_dict[k] = col

    @staticmethod
    def _add_sampling_stat(samples_info, sample_info):
        sample_info['sweep'] = torch.stack(sample_info['sweep']).mean(dim=1)
        sample_info['swap_even'] = torch.stack(sample_info['swap'][::2]).mean(dim=1)
        sample_info['swap_odd'] = torch.stack(sample_info.pop('swap')[1::2]).mean(dim=1)

        for k, v in sample_info.items():
            d = samples_info.get(k, [])
            d.append(v)
            samples_info[k] = d
